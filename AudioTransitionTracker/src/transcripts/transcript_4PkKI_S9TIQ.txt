timestamp 1: Let's wrap up our discussion of system-level
interconnect by considering how best to connect
timestamp 6: N components that need to send messages to
one another, e.g., CPUs on a multicore chip.
timestamp 13: Today such chips have a handful of cores,
but soon they may have 100s or 1000s of cores.
timestamp 19: We'll build our communications network using
point-to-point links.
timestamp 23: In our analysis, each point-to-point link
is counted at a cost of 1 hardware unit.
timestamp 28: Sending a message across a link requires one
time unit.
timestamp 32: And we'll assume that different links can
operate in parallel, so more links will mean
timestamp 36: more message traffic.
timestamp 38: We'll do an asymptotic analysis of the throughput
(total messages per unit time), latency (worst-case
timestamp 46: time to deliver a single message), and hardware
cost.
timestamp 49: In other words, we'll make a rough estimate
how these quantities change as N grows.
timestamp 57: Note that in general the throughput and hardware
cost are proportional to the number of point-to-point
timestamp 62: links.
timestamp 63: Our baseline is the backplane bus discussed
earlier, where all the components share a
timestamp 68: single communication channel.
timestamp 70: With only a single channel, bus throughput
is 1 message per unit time and a message can
timestamp 76: travel between any two components in one time
unit.
timestamp 80: Since each component has to have an interface
to the shared channel, the total hardware
timestamp 84: cost is O(n).
timestamp 87: In a ring network each component sends its
messages to a single neighbor and the links
timestamp 92: are arranged so that its possible to reach
all components.
timestamp 95: There are N links in total, so the throughput
and cost are both O(n).
timestamp 102: The worst case latency is also O(n) since
a message might have to travel across N-1
timestamp 107: links to reach the neighbor that's immediately
upstream.
timestamp 112: Ring topologies are useful when message latency
isn't important or when most messages are
timestamp 117: to the component that's immediately downstream,
i.e., the components form a processing pipeline.
timestamp 123: The most general network topology is when
every component has a direct link to every
timestamp 128: other component.
timestamp 130: There are O(N**2) links so the throughput
and cost are both O(N**2).
timestamp 135: And the latency is 1 time unit since each
destination is directly accessible.
timestamp 141: Although expensive, complete graphs offer
very high throughput with very low latencies.
timestamp 148: A variant of the complete graph is the crossbar
switch where a particular row and column can
timestamp 154: be connected to form a link between particular
A and B components
timestamp 158: with the restriction that each row and each
column can only carry 1 message during each
timestamp 163: time unit.
timestamp 164: Assume that the first row and first column
connect to the same component, and so on,
timestamp 169: i.e., that the example crossbar switch is
being used to connect 4 components.
timestamp 174: Then there are O(n) messages delivered each
time unit, with a latency of 1.
timestamp 180: There are N**2 switches in the crossbar, so
the cost is O(N**2) even though there are
timestamp 185: only O(n) links.
timestamp 188: In mesh networks, components are connected
to some fixed number of neighboring components,
timestamp 193: in either 2 or 3 dimensions.
timestamp 196: Hence the total number of links is proportional
to the number of components, so both throughput
timestamp 201: and cost are O(n).
timestamp 204: The worst-case latencies for mesh networks
are proportional to length of the sides, so
timestamp 208: the latency is O(sqrt n) for 2D meshes and
O(cube root n) for 3D meshes.
timestamp 215: The orderly layout, constant per-node hardware
costs, and modest worst-case latency make
timestamp 221: 2D 4-neighbor meshes a popular choice for
the current generation of experimental multi-core
timestamp 227: processors.
timestamp 228: Hypercube and tree networks offer logarithmic
latencies, which for large N may be faster
timestamp 234: than mesh networks.
timestamp 236: The original CM-1 Connection Machine designed
in the 80's used a hypercube network to connect
timestamp 242: up to 65,536 very simple processors, each
connected to 16 neighbors.
timestamp 250: Later generations incorporated smaller numbers
of more sophisticated processors, still connected
timestamp 255: by a hypercube network.
timestamp 258: In the early 90's the last generation of Connection
Machines used a tree network, with the clever
timestamp 263: innovation that the links towards the root
of the tree had a higher message capacity.
timestamp 269: Here's a summary of the theoretical latencies
we calculated for the various topologies.
timestamp 274: As a reality check, it's important to realize
that the lower bound on the worst-case distance
timestamp 279: between components in our 3-dimensional world
is O(cube root of N).
timestamp 283: In the case of a 2D layout, the worst-case
distance is O(sqrt N).
timestamp 289: Since we know that the time to transmit a
message is proportional to the distance traveled,
timestamp 294: we should modify our latency calculations
to reflect this physical constraint.
timestamp 300: Note that the bus and crossbar involve N connections
to a single link, so here the lower-bound
timestamp 306: on the latency needs to reflect the capacitive
load added by each connection.
timestamp 311: The winner?
timestamp 313: Mesh networks avoid the need for longer wires
as the number of connected components grows
timestamp 318: and appear to be an attractive alternative
for high-capacity communication networks connecting
timestamp 323: 1000's of processors.
timestamp 326: Summarizing our discussion:
point-to-point links are in common use today
timestamp 330: for system-level interconnect, and as a result
our systems are faster, more reliable, more
timestamp 335: energy-efficient and smaller than ever before.
timestamp 339: Multi-signal parallel buses are still used
for very-high-bandwidth connections to memories,
timestamp 344: with a lot of very careful engineering to
avoid the electrical problems observed in
timestamp 349: earlier bus implementations.
timestamp 353: Wireless connections are in common use to
connect mobile devices to nearby components
timestamp 358: and there has been interesting work on how
to allow mobile devices to discover what peripherals
timestamp 363: are nearby and enable them to connect automatically.
timestamp 367: The upcoming generation of multi-core chips
will have 10's to 100's of processing cores.
timestamp 372: There is a lot ongoing research to determine
which communication topology would offer the
timestamp 377: best combination of high communication bandwidth
and low latency.
timestamp 383: The next ten years will be an interesting
time for on-chip network engineers!
